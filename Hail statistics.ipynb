{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e79e08",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b320d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "import wrf\n",
    "from wrf import getvar, ALL_TIMES, latlon_coords, CoordPair, vertcross, to_np, interpline\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import cartopy.crs as crs\n",
    "from matplotlib.cm import get_cmap\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm \n",
    "import matplotlib as mpl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e787ef",
   "metadata": {},
   "source": [
    "### Load in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "845ffc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to import files\n",
    "\n",
    "def dataset_files(setup, run_no, year, file = True):\n",
    "    year_dict = {\n",
    "        2017: ['16_00', '15_23', '15_22', '15_21', '15_20', '2017-02'],\n",
    "        2018: ['18_00', '17_23', '17_22', '17_21', '17_20', '2018-12'],\n",
    "        2020: ['15_00', '14_23', '14_22', '14_21', '14_20', '2020-12'],\n",
    "        2024: ['11_00', '10_23', '10_22', '10_21', '10_20', '2024-02'],\n",
    "    }\n",
    "    if file:\n",
    "        t = year_dict[year][run_no-1]\n",
    "        t_ind = int(t[0:2])\n",
    "        t2 = str(t_ind+1)+t[2:]\n",
    "        t3 = str(t_ind+2)+t[2:]\n",
    "        if run_no == 1:\n",
    "            dat = [Dataset(f'/g/data/li18/em3807_2/WRF_runs/{setup}_{year}/wrfout_d02_{year_dict[year][5]}-{t}:00:00'),\n",
    "            Dataset(f'/g/data/li18/em3807_2/WRF_runs/{setup}_{year}/wrfout_d02_{year_dict[year][5]}-{t2}:00:00'),\n",
    "            Dataset(f'/g/data/li18/em3807_2/WRF_runs/{setup}_{year}/wrfout_d02_{year_dict[year][5]}-{t2}:10:00'),\n",
    "            Dataset(f'/g/data/li18/em3807_2/WRF_runs/{setup}_{year}/wrfout_d02_{year_dict[year][5]}-{t3}:10:00')]\n",
    "        else:\n",
    "            hour = t[3:5]\n",
    "            dat = [Dataset(f'/g/data/li18/em3807_2/WRF_runs/{setup}_{year}_{str(run_no)}/wrfout_d02_{year_dict[year][5]}-{t}:00:00'),\n",
    "            Dataset(f'/g/data/li18/em3807_2/WRF_runs/{setup}_{year}_{str(run_no)}/wrfout_d02_{year_dict[year][5]}-{t2}:00:00'),\n",
    "            Dataset(f'/g/data/li18/em3807_2/WRF_runs/{setup}_{year}_{str(run_no)}/wrfout_d02_{year_dict[year][5]}-{t2}:10:00'),\n",
    "            Dataset(f'/g/data/li18/em3807_2/WRF_runs/{setup}_{year}_{str(run_no)}/wrfout_d02_{year_dict[year][5]}-{t3}:10:00')]\n",
    "        return(dat)\n",
    "    else:\n",
    "        if run_no == 1:\n",
    "            dat = Dataset(f'/g/data/li18/em3807_2/WRF_runs/extracted_data/{setup}_{year}_subset.nc')\n",
    "        else:\n",
    "            dat = Dataset(f'/g/data/li18/em3807_2/WRF_runs/extracted_data/{setup}_{year}_{run_no}_subset.nc')\n",
    "        return(dat)\n",
    "\n",
    "# 2017\n",
    "\n",
    "data_17_crop = dataset_files('Cropland', 1, 2017)\n",
    "data_17_crop_2 = dataset_files('Cropland', 2, 2017)\n",
    "data_17_crop_3 = dataset_files('Cropland', 3, 2017)\n",
    "data_17_crop_4 = dataset_files('Cropland', 4, 2017)\n",
    "data_17_crop_5 = dataset_files('Cropland', 5, 2017)\n",
    "\n",
    "data_17_noeuro = dataset_files('Pre_Euro', 1, 2017, file = False)\n",
    "data_17_noeuro_2 = dataset_files('Pre_Euro', 2, 2017, file = False)\n",
    "data_17_noeuro_3 = dataset_files('Pre_Euro', 3, 2017, file = False)\n",
    "data_17_noeuro_4 = dataset_files('Pre_Euro', 4, 2017, file = False)\n",
    "data_17_noeuro_5 = dataset_files('Pre_Euro', 5, 2017, file = False)\n",
    "\n",
    "data_17_nat = dataset_files('Natland', 1, 2017, file = False)\n",
    "data_17_nat_2 = dataset_files('Natland', 2, 2017, file = False)\n",
    "data_17_nat_3 = dataset_files('Natland', 3, 2017, file = False)\n",
    "data_17_nat_4 = dataset_files('Natland', 4, 2017, file = False)\n",
    "data_17_nat_5 = dataset_files('Natland', 5, 2017, file = False)\n",
    "\n",
    "data_17_def = dataset_files('NoUCM_WRFDef', 1, 2017, file = False)\n",
    "data_17_def_2 = dataset_files('NoUCM_WRFDef', 2, 2017, file = False)\n",
    "data_17_def_3 = dataset_files('NoUCM_WRFDef', 3, 2017, file = False)\n",
    "data_17_def_4 = dataset_files('NoUCM_WRFDef', 4, 2017, file = False)\n",
    "data_17_def_5 = dataset_files('NoUCM_WRFDef', 5, 2017, file = False)\n",
    "\n",
    "data_17_defurb = dataset_files('BEPBEM_WRFDef', 1, 2017, file = False)\n",
    "data_17_defurb_2 = dataset_files('BEPBEM_WRFDef', 2, 2017, file = False)\n",
    "data_17_defurb_3 = dataset_files('BEPBEM_WRFDef', 3, 2017, file = False)\n",
    "data_17_defurb_4 = dataset_files('BEPBEM_WRFDef', 4, 2017, file = False)\n",
    "data_17_defurb_5 = dataset_files('BEPBEM_WRFDef', 5, 2017, file = False)\n",
    "\n",
    "data_17_grurb = dataset_files('BEPBEM_GrUrban', 1, 2017, file = False)\n",
    "data_17_grurb_2 = dataset_files('BEPBEM_GrUrban', 2, 2017, file = False)\n",
    "data_17_grurb_3 = dataset_files('BEPBEM_GrUrban', 3, 2017, file = False)\n",
    "data_17_grurb_4 = dataset_files('BEPBEM_GrUrban', 4, 2017, file = False)\n",
    "data_17_grurb_5 = dataset_files('BEPBEM_GrUrban', 5, 2017, file = False)\n",
    "\n",
    "data_17_gr = dataset_files('NoUCM_GrUrban', 1, 2017, file = False)\n",
    "data_17_gr_2 = dataset_files('NoUCM_GrUrban', 2, 2017, file = False)\n",
    "data_17_gr_3 = dataset_files('NoUCM_GrUrban', 3, 2017, file = False)\n",
    "data_17_gr_4 = dataset_files('NoUCM_GrUrban', 4, 2017, file = False)\n",
    "data_17_gr_5 = dataset_files('NoUCM_GrUrban', 5, 2017, file = False)\n",
    "\n",
    "# 2018\n",
    "\n",
    "data_18_crop = dataset_files('Cropland', 1, 2018)\n",
    "data_18_crop_2 = dataset_files('Cropland', 2, 2018)\n",
    "data_18_crop_3 = dataset_files('Cropland', 3, 2018)\n",
    "data_18_crop_4 = dataset_files('Cropland', 4, 2018)\n",
    "data_18_crop_5 = dataset_files('Cropland', 5, 2018, file = False)\n",
    "\n",
    "data_18_noeuro = dataset_files('Pre_Euro', 1, 2018, file = False)\n",
    "data_18_noeuro_2 = dataset_files('Pre_Euro', 2, 2018, file = False)\n",
    "data_18_noeuro_3 = dataset_files('Pre_Euro', 3, 2018, file = False)\n",
    "data_18_noeuro_4 = dataset_files('Pre_Euro', 4, 2018, file = False)\n",
    "data_18_noeuro_5 = dataset_files('Pre_Euro', 5, 2018, file = False)\n",
    "\n",
    "data_18_nat = dataset_files('Natland', 1, 2018, file = False)\n",
    "data_18_nat_2 = dataset_files('Natland', 2, 2018, file = False)\n",
    "data_18_nat_3 = dataset_files('Natland', 3, 2018, file = False)\n",
    "data_18_nat_4 = dataset_files('Natland', 4, 2018, file = False)\n",
    "data_18_nat_5 = dataset_files('Natland', 5, 2018, file = False)\n",
    "\n",
    "data_18_def = dataset_files('NoUCM_WRFDef', 1, 2018, file = False)\n",
    "data_18_def_2 = dataset_files('NoUCM_WRFDef', 2, 2018, file = False)\n",
    "data_18_def_3 = dataset_files('NoUCM_WRFDef', 3, 2018, file = False)\n",
    "data_18_def_4 = dataset_files('NoUCM_WRFDef', 4, 2018, file = False)\n",
    "data_18_def_5 = dataset_files('NoUCM_WRFDef', 5, 2018, file = False)\n",
    "\n",
    "data_18_defurb = dataset_files('BEPBEM_WRFDef', 1, 2018, file = False)\n",
    "data_18_defurb_2 = dataset_files('BEPBEM_WRFDef', 2, 2018, file = False)\n",
    "data_18_defurb_3 = dataset_files('BEPBEM_WRFDef', 3, 2018, file = False)\n",
    "data_18_defurb_4 = dataset_files('BEPBEM_WRFDef', 4, 2018, file = False)\n",
    "data_18_defurb_5 = dataset_files('BEPBEM_WRFDef', 5, 2018, file = False)\n",
    "\n",
    "data_18_grurb = dataset_files('BEPBEM_GrUrban', 1, 2018, file = False)\n",
    "data_18_grurb_2 = dataset_files('BEPBEM_GrUrban', 2, 2018, file = False)\n",
    "data_18_grurb_3 = dataset_files('BEPBEM_GrUrban', 3, 2018, file = False)\n",
    "data_18_grurb_4 = dataset_files('BEPBEM_GrUrban', 4, 2018, file = False)\n",
    "data_18_grurb_5 = dataset_files('BEPBEM_GrUrban', 5, 2018, file = False)\n",
    "\n",
    "data_18_gr = dataset_files('NoUCM_GrUrban', 1, 2018, file = False)\n",
    "data_18_gr_2 = dataset_files('NoUCM_GrUrban', 2, 2018, file = False)\n",
    "data_18_gr_3 = dataset_files('NoUCM_GrUrban', 3, 2018, file = False)\n",
    "data_18_gr_4 = dataset_files('NoUCM_GrUrban', 4, 2018, file = False)\n",
    "data_18_gr_5 = dataset_files('NoUCM_GrUrban', 5, 2018, file = False)\n",
    "\n",
    "# 2020\n",
    "\n",
    "data_20_crop = dataset_files('Cropland', 1, 2020)\n",
    "data_20_crop_2 = dataset_files('Cropland', 2, 2020)\n",
    "data_20_crop_3 = dataset_files('Cropland', 3, 2020)\n",
    "data_20_crop_4 = dataset_files('Cropland', 4, 2020)\n",
    "data_20_crop_5 = dataset_files('Cropland', 5, 2020)\n",
    "\n",
    "data_20_noeuro = dataset_files('Pre_Euro', 1, 2020, file = False)\n",
    "data_20_noeuro_2 = dataset_files('Pre_Euro', 2, 2020, file = False)\n",
    "data_20_noeuro_3 = dataset_files('Pre_Euro', 3, 2020, file = False)\n",
    "data_20_noeuro_4 = dataset_files('Pre_Euro', 4, 2020, file = False)\n",
    "data_20_noeuro_5 = dataset_files('Pre_Euro', 5, 2020, file = False)\n",
    "\n",
    "data_20_nat = dataset_files('Natland', 1, 2020, file = False)\n",
    "data_20_nat_2 = dataset_files('Natland', 2, 2020, file = False)\n",
    "data_20_nat_3 = dataset_files('Natland', 3, 2020, file = False)\n",
    "data_20_nat_4 = dataset_files('Natland', 4, 2020, file = False)\n",
    "data_20_nat_5 = dataset_files('Natland', 5, 2020, file = False)\n",
    "\n",
    "data_20_def = dataset_files('NoUCM_WRFDef', 1, 2020, file = False)\n",
    "data_20_def_2 = dataset_files('NoUCM_WRFDef', 2, 2020, file = False)\n",
    "data_20_def_3 = dataset_files('NoUCM_WRFDef', 3, 2020, file = False)\n",
    "data_20_def_4 = dataset_files('NoUCM_WRFDef', 4, 2020, file = False)\n",
    "data_20_def_5 = dataset_files('NoUCM_WRFDef', 5, 2020, file = False)\n",
    "\n",
    "data_20_defurb = dataset_files('BEPBEM_WRFDef', 1, 2020, file = False)\n",
    "data_20_defurb_2 = dataset_files('BEPBEM_WRFDef', 2, 2020, file = False)\n",
    "data_20_defurb_3 = dataset_files('BEPBEM_WRFDef', 3, 2020, file = False)\n",
    "data_20_defurb_4 = dataset_files('BEPBEM_WRFDef', 4, 2020, file = False)\n",
    "data_20_defurb_5 = dataset_files('BEPBEM_WRFDef', 5, 2020, file = False)\n",
    "\n",
    "data_20_grurb = dataset_files('BEPBEM_GrUrban', 1, 2020, file = False)\n",
    "data_20_grurb_2 = dataset_files('BEPBEM_GrUrban', 2, 2020, file = False)\n",
    "data_20_grurb_3 = dataset_files('BEPBEM_GrUrban', 3, 2020, file = False)\n",
    "data_20_grurb_4 = dataset_files('BEPBEM_GrUrban', 4, 2020, file = False)\n",
    "data_20_grurb_5 = dataset_files('BEPBEM_GrUrban', 5, 2020, file = False)\n",
    "\n",
    "data_20_gr = dataset_files('NoUCM_GrUrban', 1, 2020, file = False)\n",
    "data_20_gr_2 = dataset_files('NoUCM_GrUrban', 2, 2020, file = False)\n",
    "data_20_gr_3 = dataset_files('NoUCM_GrUrban', 3, 2020, file = False)\n",
    "data_20_gr_4 = dataset_files('NoUCM_GrUrban', 4, 2020, file = False)\n",
    "data_20_gr_5 = dataset_files('NoUCM_GrUrban', 5, 2020, file = False)\n",
    "\n",
    "# 2024\n",
    "\n",
    "data_24_crop = dataset_files('Cropland', 1, 2024)\n",
    "data_24_crop_2 = dataset_files('Cropland', 2, 2024)\n",
    "data_24_crop_3 = dataset_files('Cropland', 3, 2024)\n",
    "data_24_crop_4 = dataset_files('Cropland', 4, 2024)\n",
    "data_24_crop_5 = dataset_files('Cropland', 5, 2024)\n",
    "\n",
    "data_24_noeuro = dataset_files('Pre_Euro', 1, 2024)\n",
    "data_24_noeuro_2 = dataset_files('Pre_Euro', 2, 2024)\n",
    "data_24_noeuro_3 = dataset_files('Pre_Euro', 3, 2024)\n",
    "data_24_noeuro_4 = dataset_files('Pre_Euro', 4, 2024)\n",
    "data_24_noeuro_5 = dataset_files('Pre_Euro', 5, 2024)\n",
    "\n",
    "data_24_nat = dataset_files('Natland', 1, 2024)\n",
    "data_24_nat_2 = dataset_files('Natland', 2, 2024)\n",
    "data_24_nat_3 = dataset_files('Natland', 3, 2024)\n",
    "data_24_nat_4 = dataset_files('Natland', 4, 2024)\n",
    "data_24_nat_5 = dataset_files('Natland', 5, 2024)\n",
    "\n",
    "data_24_def = dataset_files('NoUCM_WRFDef', 1, 2024)\n",
    "data_24_def_2 = dataset_files('NoUCM_WRFDef', 2, 2024)\n",
    "data_24_def_3 = dataset_files('NoUCM_WRFDef', 3, 2024)\n",
    "data_24_def_4 = dataset_files('NoUCM_WRFDef', 4, 2024)\n",
    "data_24_def_5 = dataset_files('NoUCM_WRFDef', 5, 2024)\n",
    "\n",
    "data_24_defurb = dataset_files('BEPBEM_WRFDef', 1, 2024)\n",
    "data_24_defurb_2 = dataset_files('BEPBEM_WRFDef', 2, 2024)\n",
    "data_24_defurb_3 = dataset_files('BEPBEM_WRFDef', 3, 2024)\n",
    "data_24_defurb_4 = dataset_files('BEPBEM_WRFDef', 4, 2024)\n",
    "data_24_defurb_5 = dataset_files('BEPBEM_WRFDef', 5, 2024)\n",
    "\n",
    "data_24_grurb = dataset_files('BEPBEM_GrUrban', 1, 2024)\n",
    "data_24_grurb_2 = dataset_files('BEPBEM_GrUrban', 2, 2024)\n",
    "data_24_grurb_3 = dataset_files('BEPBEM_GrUrban', 3, 2024)\n",
    "data_24_grurb_4 = dataset_files('BEPBEM_GrUrban', 4, 2024)\n",
    "data_24_grurb_5 = dataset_files('BEPBEM_GrUrban', 5, 2024)\n",
    "\n",
    "data_24_gr = dataset_files('NoUCM_GrUrban', 1, 2024)\n",
    "data_24_gr_2 = dataset_files('NoUCM_GrUrban', 2, 2024)\n",
    "data_24_gr_3 = dataset_files('NoUCM_GrUrban', 3, 2024)\n",
    "data_24_gr_4 = [Dataset('/g/data/li18/em3807_2/WRF_runs/NoUCM_GrUrban_2024_4/wrfout_d02_2024-02-10_21:00:00'),\n",
    "Dataset('/g/data/li18/em3807_2/WRF_runs/NoUCM_GrUrban_2024_4/wrfout_d02_2024-02-10_22:00:00'),\n",
    "Dataset('/g/data/li18/em3807_2/WRF_runs/NoUCM_GrUrban_2024_4/wrfout_d02_2024-02-11_21:10:00'),\n",
    "Dataset('/g/data/li18/em3807_2/WRF_runs/NoUCM_GrUrban_2024_4/wrfout_d02_2024-02-12_21:10:00')]\n",
    "data_24_gr_5 = dataset_files('NoUCM_GrUrban', 5, 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f2920",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2e444c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match UTC time to WRF index\n",
    "def match_tti(dataset, date, time):\n",
    "    timelist = wrf.extract_times(dataset, timeidx = ALL_TIMES)\n",
    "    if len(time) ==7:\n",
    "        timestr = str(date)+\"T0\"+str(time)+\".000000000\"\n",
    "    if len(time) ==8:\n",
    "        timestr = str(date)+\"T\"+str(time)+\".000000000\"\n",
    "    if isinstance(dataset, list):\n",
    "        for file in dataset:\n",
    "            if timestr in str(wrf.extract_times(file, timeidx = ALL_TIMES)):\n",
    "                file_num = dataset.index(file)\n",
    "    else:\n",
    "        file_num = ''\n",
    "    for idx, t in enumerate(timelist):\n",
    "        if timestr == str(t):\n",
    "            ind = idx\n",
    "            break\n",
    "    else:\n",
    "        return print(\"Time not in dataset\")\n",
    "    return(ind,file_num,f'timeidx: {ind} for {t}')\n",
    "\n",
    "# Convert UTC to AEST\n",
    "def utc_to_aest(timestr):\n",
    "    utc = pd.Timestamp(timestr,tz='UTC')\n",
    "    aus = utc.tz_convert(tz='Australia/Sydney')\n",
    "    date = str(aus).split(' ')[0]\n",
    "    time = str(aus).split(' ')[1][:8]\n",
    "    wrfstr = date+\"T\"+time+\".000000000\"\n",
    "    aest = time+' '+date\n",
    "    return(wrfstr,aest,f'AEST is {time} {date}')\n",
    "\n",
    "# Convert AEST to UTC\n",
    "def aest_to_utc(date,time):\n",
    "    datetime = f'{date} {time}'\n",
    "    austime = pd.Timestamp(datetime,tz='Australia/Sydney')\n",
    "    utctime = austime.tz_convert(tz='UTC')\n",
    "    udate = str(utctime).split(' ')[0]\n",
    "    utime = str(utctime).split(' ')[1][:8] \n",
    "    wrfstr = udate+\"T\"+utime+\".000000000\"  \n",
    "    return(wrfstr,[udate, utime])\n",
    "\n",
    "# Find the number of grid cells with hail\n",
    "def grid_number(var):\n",
    "    land = wrf.getvar(data_24_gr, 'LU_INDEX', timeidx=8, method='cat')\n",
    "    varurban = np.where(land <= 49, 0, var)\n",
    "    urbanlist = []\n",
    "    gridnumlist = []\n",
    "    for i in range (0, len(var)):\n",
    "        gridnumlist.append(np.count_nonzero(var[i]))\n",
    "    for i in range (0, len(var)):\n",
    "        urbanlist.append(np.count_nonzero(varurban[i]))\n",
    "    gridnum = np.array(gridnumlist)\n",
    "    urbnum = np.array(urbanlist)\n",
    "    return(gridnum,urbnum)\n",
    "\n",
    "# Calculate percentage coverage over different regions\n",
    "def percentcov(var):\n",
    "    land = wrf.getvar(data_24_gr, 'LU_INDEX', timeidx=8, method='cat')\n",
    "    urbland = np.where(land <= 49, 0, land)\n",
    "    urb = np.where(land <= 49, 0, land)\n",
    "    variable = np.where(var == 49, 0, var)\n",
    "    urb[0:140, 0:200] = 0 # area below Sydney\n",
    "    urb[0:289, 0:150] = 0 # left of Sydney\n",
    "    urb[205:289, 0:279] = 0 # left of Sydney\n",
    "    metarea = np.count_nonzero(urb)\n",
    "    totarea = len(urbland)*len(urbland[0])\n",
    "    urbarea = np.count_nonzero(urbland)\n",
    "    vararea = np.count_nonzero(var)\n",
    "    urbvar = np.where(urbland == 0, 0, var)\n",
    "    metvar = np.where(urb == 0, 0, var)\n",
    "    urbvararea = np.count_nonzero(urbvar)\n",
    "    metvararea = np.count_nonzero(metvar)\n",
    "    return({'Urban area': [f'{round(float(urbvararea/urbarea)*100,1)}%',round(float(urbvar.max()),2)],\n",
    "            'Sydney metropolitan': [f'{round(float(metvararea/metarea)*100,1)}%',round(float(metvar.max()),2)],\n",
    "            'Total domain': [f'{round(float(vararea/totarea)*100,1)}%', round(float(variable.max()),2)]})\n",
    "\n",
    "# Get WRF indices for an ensemble\n",
    "def get_indexes(caselist,start,end): #start date (sd), start hour (sh), end date (ed) and end hour (eh)\n",
    "    indl = []\n",
    "    for case in caselist:\n",
    "        inds = match_tti(case, aest_to_utc(start[0],start[1])[1][0], aest_to_utc(start[0],start[1])[1][1])[0]\n",
    "        inde = match_tti(case, aest_to_utc(end[0],end[1])[1][0], aest_to_utc(end[0],end[1])[1][1])[0]\n",
    "        indl.append([inds,inde])\n",
    "    return indl\n",
    "\n",
    "# Define the urban outline in the gridded dataset\n",
    "def urb_outline():\n",
    "    grland = wrf.getvar(data_24_gr, 'LU_INDEX', timeidx=0, method='cat')\n",
    "    urb = np.where(grland <= 49, 0, grland) #set everything that isn't urban to zero\n",
    "    uniurb = np.where(urb >= 49, 1, urb) #make urban area uniform\n",
    "    return uniurb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63206d1f",
   "metadata": {},
   "source": [
    "### Setting up ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f966de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "en1_17 = [data_17_crop, data_17_noeuro, data_17_nat, data_17_gr, data_17_grurb, data_17_def, data_17_defurb]\n",
    "en2_17 = [data_17_crop_2, data_17_noeuro_2, data_17_nat_2, data_17_gr_2, data_17_grurb_2, data_17_def_2, data_17_defurb_2]\n",
    "en3_17 = [data_17_crop_3, data_17_noeuro_3, data_17_nat_3, data_17_gr_3, data_17_grurb_3, data_17_def_3, data_17_defurb_3]\n",
    "en4_17 = [data_17_crop_4, data_17_noeuro_4, data_17_nat_4, data_17_gr_4, data_17_grurb_4, data_17_def_4, data_17_defurb_4]\n",
    "en5_17 = [data_17_crop_5, data_17_noeuro_5, data_17_nat_5, data_17_gr_5, data_17_grurb_5, data_17_def_5, data_17_defurb_5]\n",
    "\n",
    "en1_18 = [data_18_crop, data_18_noeuro, data_18_nat, data_18_gr, data_18_grurb, data_18_def, data_18_defurb]\n",
    "en2_18 = [data_18_crop_2, data_18_noeuro_2, data_18_nat_2, data_18_gr_2, data_18_grurb_2, data_18_def_2, data_18_defurb_2]\n",
    "en3_18 = [data_18_crop_3, data_18_noeuro_3, data_18_nat_3, data_18_gr_3, data_18_grurb_3, data_18_def_3, data_18_defurb_3]\n",
    "en4_18 = [data_18_crop_4, data_18_noeuro_4, data_18_nat_4, data_18_gr_4, data_18_grurb_4, data_18_def_4, data_18_defurb_4]\n",
    "en5_18 = [data_18_crop_5, data_18_noeuro_5, data_18_nat_5, data_18_gr_5, data_18_grurb_5, data_18_def_5, data_18_defurb_5]\n",
    "\n",
    "en1_20 = [data_20_crop, data_20_noeuro, data_20_nat, data_20_gr, data_20_grurb, data_20_def, data_20_defurb]\n",
    "en2_20 = [data_20_crop_2, data_20_noeuro_2, data_20_nat_2, data_20_gr_2, data_20_grurb_2, data_20_def_2, data_20_defurb_2]\n",
    "en3_20 = [data_20_crop_3, data_20_noeuro_3, data_20_nat_3, data_20_gr_3, data_20_grurb_3, data_20_def_3, data_20_defurb_3]\n",
    "en4_20 = [data_20_crop_4, data_20_noeuro_4, data_20_nat_4, data_20_gr_4, data_20_grurb_4, data_20_def_4, data_20_defurb_4]\n",
    "en5_20 = [data_20_crop_5, data_20_noeuro_5, data_20_nat_5, data_20_gr_5, data_20_grurb_5, data_20_def_5, data_20_defurb_5]\n",
    "\n",
    "en1_24 = [data_24_crop, data_24_noeuro, data_24_nat, data_24_gr, data_24_grurb, data_24_def, data_24_defurb]\n",
    "en2_24 = [data_24_crop_2, data_24_noeuro_2, data_24_nat_2, data_24_gr_2, data_24_grurb_2, data_24_def_2, data_24_defurb_2]\n",
    "en3_24 = [data_24_crop_3, data_24_noeuro_3, data_24_nat_3, data_24_gr_3, data_24_grurb_3, data_24_def_3, data_24_defurb_3]\n",
    "en4_24 = [data_24_crop_4, data_24_noeuro_4, data_24_nat_4, data_24_gr_4, data_24_grurb_4, data_24_def_4, data_24_defurb_4]\n",
    "en5_24 = [data_24_crop_5, data_24_noeuro_5, data_24_nat_5, data_24_gr_5, data_24_grurb_5, data_24_def_5, data_24_defurb_5]\n",
    "\n",
    "ensemble_dict = { 2017: [en1_17, en2_17, en3_17, en4_17, en5_17],\n",
    "2018: [en1_18, en2_18, en3_18, en4_18, en5_18],\n",
    "2020: [en1_20, en2_20, en3_20, en4_20, en5_20],\n",
    "2024: [en1_24, en2_24, en3_24, en4_24, en5_24]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbd1ec1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRID NUM COVERAGE\n",
      "Cropland: 12930\n",
      "Pre-European: 13239\n",
      "Forest: 13379\n",
      "Gridded No-UCM: 12211\n",
      "Gridded BEP-BEM: 11903\n",
      "Default No-UCM: 12680\n",
      "Default BEP-BEM: 13517\n",
      "PERCENTAGE COVERAGE\n",
      "Cropland: {'Urban area': ['22.7%', 0.03], 'Sydney metropolitan': ['16.2%', 0.02], 'Total domain': ['15.9%', 0.05]}\n",
      "Pre-European: {'Urban area': ['28.1%', 0.03], 'Sydney metropolitan': ['25.7%', 0.03], 'Total domain': ['16.3%', 0.05]}\n",
      "Forest: {'Urban area': ['26.8%', 0.04], 'Sydney metropolitan': ['20.2%', 0.03], 'Total domain': ['16.5%', 0.05]}\n",
      "Gridded No-UCM: {'Urban area': ['22.6%', 0.03], 'Sydney metropolitan': ['19.3%', 0.03], 'Total domain': ['15.0%', 0.04]}\n",
      "Gridded BEP-BEM: {'Urban area': ['27.4%', 0.03], 'Sydney metropolitan': ['22.7%', 0.03], 'Total domain': ['14.7%', 0.04]}\n",
      "Default No-UCM: {'Urban area': ['23.3%', 0.03], 'Sydney metropolitan': ['15.1%', 0.02], 'Total domain': ['15.6%', 0.04]}\n",
      "Default BEP-BEM: {'Urban area': ['26.1%', 0.03], 'Sydney metropolitan': ['19.1%', 0.02], 'Total domain': ['16.6%', 0.04]}\n",
      "MAX HAIL SIZE\n",
      "Cropland: 4.7 cm\n",
      "Pre-European: 5.03 cm\n",
      "Forest: 5.14 cm\n",
      "Gridded No-UCM: 4.18 cm\n",
      "Gridded BEP-BEM: 3.77 cm\n",
      "Default No-UCM: 4.46 cm\n",
      "Default BEP-BEM: 3.8 cm\n"
     ]
    }
   ],
   "source": [
    "# Select ensemble member\n",
    "en = en1_24\n",
    "\n",
    "# Set storm date\n",
    "start_date = '2024-02-13' #local date\n",
    "start_hour = '00:00:00'   #local time\n",
    "\n",
    "end_date = '2024-02-14' #local date\n",
    "end_hour = '00:00:00'   #local time\n",
    "\n",
    "# Find indices for the storm in the WRF dataset\n",
    "inds = get_indexes(en,[start_date,start_hour],[end_date,end_hour])\n",
    "namelist = ['Cropland','Pre-European', 'Forest', 'Gridded No-UCM', 'Gridded BEP-BEM','Default No-UCM', 'Default BEP-BEM']\n",
    "\n",
    "# Extract surface hail from the selected ensemble\n",
    "hail = []\n",
    "for caseind in range(len(inds)):\n",
    "    h = wrf.getvar(en[caseind], 'HAIL_MAXK1', timeidx=ALL_TIMES, method='cat')[inds[caseind][0]:inds[caseind][1]+1,:,:].max('Time')\n",
    "    hail.append(h)\n",
    "\n",
    "# Print hail statistics\n",
    "print('GRID NUM COVERAGE')\n",
    "for ind in range(len(hail)):\n",
    "    val =np.sum(grid_number(hail[ind])[0])\n",
    "    print(f\"{namelist[ind]}: {val}\")\n",
    "print('PERCENTAGE COVERAGE')\n",
    "for ind in range(len(hail)):\n",
    "    val = percentcov(hail[ind])\n",
    "    print(f\"{namelist[ind]}: {val}\")\n",
    "print('MAX HAIL SIZE')\n",
    "for ind in range(len(hail)):\n",
    "    urb_hail = np.where(urb_outline() == 0, 0, hail)\n",
    "    print(f\"{namelist[ind]}: {round(hail[ind].max(['south_north','west_east']).values*100,2)} cm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis3-25.05",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
